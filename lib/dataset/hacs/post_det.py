import os
import sys
import numpy as np
import pandas as pd
import json
import time
import argparse
import multiprocessing as mp
import pickle
import torch

from lib.utils.util import load_config
from lib.utils.tool import boundary_choose


def soft_nms(df, alpha=0.5, t1=0, t2=0, max_num=100):
    """
    df: proposals generated by network;
    alpha: alpha value of Gaussian decaying function;
    t1, t2: threshold for soft nms.
    """
    df = df.sort_values(by="score", ascending=False)
    tstart = list(df.xmin.values[:])
    tend = list(df.xmax.values[:])
    tscore = list(df.score.values[:])

    rstart = []
    rend = []
    rscore = []

    while len(tscore) > 1 and len(rscore) < max_num:
        max_index = tscore.index(max(tscore))
        tmp_start = tstart[max_index]
        tmp_end = tend[max_index]
        tmp_score = tscore[max_index]
        rstart.append(tmp_start)
        rend.append(tmp_end)
        rscore.append(tmp_score)
        tstart.pop(max_index)
        tend.pop(max_index)
        tscore.pop(max_index)

        tstart = np.array(tstart)
        tend = np.array(tend)
        tscore = np.array(tscore)

        tt1 = np.maximum(tmp_start, tstart)
        tt2 = np.minimum(tmp_end, tend)
        intersection = tt2 - tt1
        duration = tend - tstart
        tmp_width = tmp_end - tmp_start
        iou = intersection / (tmp_width + duration - intersection).astype(np.float)

        idxs = np.where(iou > t1 + (t2 - t1) * tmp_width)[0]
        tscore[idxs] = tscore[idxs] * np.exp(-np.square(iou[idxs]) / alpha)

        tstart = list(tstart)
        tend = list(tend)
        tscore = list(tscore)

    newDf = pd.DataFrame()
    newDf["score"] = rscore
    newDf["xmin"] = rstart
    newDf["xmax"] = rend
    return newDf


def getDatasetDict():
    json_data = json.load(open("./lib/dataset/hacs/data/HACS_segments_v1.1.1.json"))
    database = json_data["database"]

    train_dict = {}
    val_dict = {}
    test_dict = {}
    for video_name in list(database.keys()):
        video_info = database[video_name]
        if video_info["subset"] == "training":
            train_dict[video_name] = video_info
        elif video_info["subset"] == "validation":
            val_dict[video_name] = video_info
        elif video_info["subset"] == "testing":
            test_dict[video_name] = video_info
    return train_dict, val_dict, test_dict


def _gen_detection_video(video_list, video_dict, cls_data, cfg, num_prop=100, topk=3):
    tscale = cfg.DATASET.tscale
    dscale = cfg.DATASET.dscale
    output_path = "./exps/{}/output/".format(cfg.EXP_NAME)

    anchor_xmin = np.array([i / tscale for i in range(tscale)])
    anchor_xmax = np.array([i / tscale for i in range(1, tscale + 1)])
    cols = ["xmin", "xmax", "score"]
    cls_data_score = cls_data["results"]
    cls_data_action = cls_data["class"]

    for video_name in video_list:
        file_path = os.path.join(output_path, "{}.pkl".format(video_name))

        with open(file_path, "rb") as infile:
            result = pickle.load(infile)

        [_, pred_local_s, pred_local_e, pred_global_s, pred_global_e, pred_iou_map] = result

        pred_s = np.sqrt(pred_local_s * pred_global_s)
        pred_e = np.sqrt(pred_local_e * pred_global_e)
        pred_iou_map = pred_iou_map[0, :, :] * pred_iou_map[1, :, :]

        start_mask = boundary_choose(pred_s)
        start_mask[0] = 1.0
        end_mask = boundary_choose(pred_e)
        end_mask[-1] = 1.0

        score_vector_list = []
        for idx in range(dscale):
            for jdx in range(tscale - idx):
                start_idx = jdx
                end_idx = start_idx + idx
                if start_mask[start_idx] == 1 and end_mask[end_idx] == 1:
                    xmin = anchor_xmin[start_idx]
                    xmax = anchor_xmax[end_idx]
                    conf_score = pred_iou_map[idx, jdx]
                    score_vector_list.append([xmin, xmax, conf_score])

        score_vector_list = np.stack(score_vector_list)
        df = pd.DataFrame(score_vector_list, columns=cols)

        if len(df) > 1:
            df = soft_nms(
                df,
                alpha=cfg.DETECTION_POST.alpha,
                t1=cfg.DETECTION_POST.t1,
                t2=cfg.DETECTION_POST.t2,
                max_num=num_prop,
            )
        df = df.sort_values(by="score", ascending=False)

        # sort video classification
        cls_score = np.array(cls_data_score[video_name][0])
        cls_score = np.exp(cls_score) / np.sum(np.exp(cls_score)) * 2.0
        cls_data_action = np.array(cls_data_action)
        cls_classes = cls_data_action[np.argsort(-cls_score)]
        cls_score = cls_score[np.argsort(-cls_score)]

        video_duration = float(video_dict[video_name]["duration"])
        proposal_list = []
        for j in range(min(num_prop, len(df))):
            for k in range(topk):
                tmp_proposal = {}
                tmp_proposal["label"] = cls_classes[k]
                tmp_proposal["score"] = df.score.values[j] * cls_score[k]
                tmp_proposal["segment"] = [
                    max(0, df.xmin.values[j]) * video_duration,
                    min(1, df.xmax.values[j]) * video_duration,
                ]
                proposal_list.append(tmp_proposal)
        result_dict[video_name] = proposal_list


def gen_detection_multicore(cfg):
    # get video list
    train_dict, val_dict, test_dict = getDatasetDict()
    video_dict = val_dict  # val_dict  test_dict
    video_list = list(video_dict.keys())

    # detection_result
    cls_data = json.load(open("./lib/dataset/hacs/data/validation94.32.json"))

    global result_dict
    result_dict = mp.Manager().dict()

    # multi processing
    pp_num = 32
    num_videos = len(video_list)
    num_videos_per_thread = num_videos / pp_num
    processes = []
    for tid in range(pp_num - 1):
        tmp_video_list = video_list[int(tid * num_videos_per_thread) : int((tid + 1) * num_videos_per_thread)]
        p = mp.Process(
            target=_gen_detection_video,
            args=(tmp_video_list, video_dict, cls_data, cfg),
        )
        p.start()
        processes.append(p)
    tmp_video_list = video_list[int((pp_num - 1) * num_videos_per_thread) :]
    p = mp.Process(
        target=_gen_detection_video,
        args=(tmp_video_list, video_dict, cls_data, cfg),
    )

    p.start()
    processes.append(p)
    for p in processes:
        p.join()

    # write file
    result_dict = dict(result_dict)
    output_dict = {
        "version": "ActivityNet 1.3",
        "results": result_dict,
        "external_data": {},
    }

    with open(cfg.result_path, "w") as out:
        json.dump(output_dict, out)


def detection_post(cfg):
    cfg.output_path = "./exps/%s/output/" % (cfg.EXP_NAME)
    cfg.result_path = "./exps/%s/result_detection.json" % (cfg.EXP_NAME)

    # post processing
    t1 = time.time()
    print("\nDetection task post processing start")
    gen_detection_multicore(cfg)
    t2 = time.time()
    print("Detection task Post processing finished, time=%.1fmins\n" % ((t2 - t1) / 60))

    # evaluation
    from lib.eval.eval_detection import ANETdetection

    tious = np.linspace(0.5, 0.95, 10)
    anet_detection = ANETdetection(
        ground_truth_filename="./lib/dataset/hacs/data/HACS_segments_v1.1.1.json",
        prediction_filename=cfg.result_path,
        tiou_thresholds=tious,
        subset="validation",
        pp_num=32,
    )

    mAPs, average_mAP = anet_detection.evaluate()

    for (tiou, mAP) in zip(tious, mAPs):
        if tiou in [0.5, 0.75, 0.95]:
            print("mAP at tIoU {:.2f} is {:.2f}%".format(tiou, mAP * 100))

    # save to file
    cfg.eval_path = "./exps/%s/results.txt" % (cfg.EXP_NAME)
    f2 = open(cfg.eval_path, "a")
    f2.write("Average-mAP: {}\n".format(average_mAP))
    for (tiou, mAP) in zip(tious, mAPs):
        f2.write("mAP at tIoU {:.2f} is {:.2f}%\n".format(tiou, mAP * 100))
    f2.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="BMN")
    parser.add_argument(
        "config",
        metavar="FILE",
        help="path to config file",
        type=str,
    )
    args = parser.parse_args()

    # load settings
    cfg = load_config(args.config)

    # post process
    detection_post(cfg)
